# CT2CT: Pix2Pix Models applied to Data Augmentation in CT Scan Settings
# Version 0.1.0-alpha -> [changelog](/changelog.md)

- If you want to read the Spanish version click [here](#spanish-version)
- Si quieres leer la versi√≥n en Espa√±ol haz click [aqu√≠](#spanish-version)

This project was mentioned in the AI YouTube channel DotCSV! ([link](https://youtu.be/BerOC6n8j9Q?t=1011) to the video)

# üíÇ‚Äç‚ôÇÔ∏èENGLISH VERSION
# CT2CT: Resolution Augmentation of Computed Tomography (CT) Scans

## üìùAbstract
The reconstruction of 3D encephala requires to perform the patient a Computed Tomography exam so that "slices" or sections of their head are obtained. The use of conventional systems usually extracts between 100 and 300 slices per patient, higher numbers would imply a longer exposure of the patient to radiation. A 2009 study of medical centers in the San Francisco Bay Area calculated that there is one extra case of cancer for every 400 to 2,000 routine chest CT exams. This has been linked to the abusive execution of the phenomena known as "defensive medicine", meaning that many CT Scans are not necessary and its overuse can potentially harm the patient if performed repeatedly under the said practice.
Using the CT2CT system (based on Pix2Pix technology), it is now possible to achieve a high number of slices of the encephalon only by scanning a lower number of "key slices" that allow the model to predict the non-scanned parts of the brain. This way, the time of exposure to radiation gets reduced significantly and the number of 2D slices can even be increased by a factor of 10.

Original complete scan (240 frames) | Original Slices + Predictions (480 frames, 2x original)
---------------------- | -----------------------------
![Original Scan gif](/assets/original.gif) | ![Predicted Scan gif](/assets/results.gif)

## ‚öôÔ∏èHow it works?
The CT2CT model obtains non-scanned intermediate layers from known scanned ones in order to augment the resolution of the 3D reconstruction of enchapla without requiring the patient to stay longer times exposed to radiation during medical exams.
Once the training has been completed, predictions of the brain structure of a patient can be asked to the model as many times as required (bearing in mind that little errors always add up).

## üèãÔ∏èTraining
The training proccess works by choosing 3 known sections (A, B and C), the input for the generator is a merge of A and C (AmC) and the output obtained is compared with the discriminator unit to B.

Layer A | Layer B | Layer C
-------|--------|--------
![Layer A](/assets/A.jpg) | ![Layer B](/assets/B.jpg) | ![Layer C](/assets/C.jpg)

This way, the generator learns how to create images of intermediate slices of the encephalon since the information of A and C is contained in the merge of both images (AmC).

Input AmC | Prediction (10 epochs)[X] | Prediction (50 epochs)[X]
----------|-------------------------|------------------------
![Image AmC](/assets/AmC.jpg) | ![Prediction 10 epochs](/assets/B_10e.jpg) | ![Prediction 50 epochs](/assets/B_50e.jpg)

[X]: Generated from other AmC not showed here.

The model has been designed so that it only requires less than 100 slices (all results shown have been generated with only 80 images of training). 50 epochs have demonstrated a huge capacity for prediction.

## ‚úÖEvaluation
CT2CT uses 20% (20 images in shown results) of the images of the dataset for evaluating the model.

## üßîHow can I use CT2CT?
Support for custom scans is possible but not easy to use unless you know how the code works. That feature will be added in a near future but you can follow a step-by-step guide in the CT2CT.ipynb to train a model, understand the process, see the results, and save the model.
I am currently developing the user-friendly dekstop version for getting a complete predicted scan.

## ‚ùóGoogle Colaboratory
The code has been written in Google Colaboratory. If you want to use this software, please open CT2CT.ipynb in Google Colaboratory.

## ‚ÑπÔ∏èExtra information
During the process of the development of this software some interesting ideas have been raised for the future:
  Image 1: When all the predicted layers are joined together, a scanner of similar resolution of the original is obtained with the only difference that the represented encephalon DOES NOT EXIST. This is an interesting idea since the head that could be extracted in 3D would be based on the original patient's head, but completely new. Face could also be reconstructed and would also be based on a person but not exactly THAT person.
  Image 2: If the images from the dataset contain descriptive HUD, the software would understand it as part of the image and will try to also predict that text. This way, data that does not change and is shown, such as the name of the patient or the date of the exam, are reproduced perfectly. But data that is constantly changing like slice number or distance are predicted in a blury way. This happens because that piece of information is unkown to the software and to the user.
  
  | Image 1 | Image 2 |
  | --- | --- |
  | 3D Predicted Reconstruction | "Blury" HUD (upper left corner) |
  | ![Predicted face](/assets/face.jpg) | ![blury HUD](/assets/B_prediction.jpg) |
  
# üíÉSPANISH VERSION
# CT2CT: Aumentaci√≥n de la Resoluci√≥n de Esc√°neres de Tomograf√≠a Computada (CT)

## üìùAbstracto
La reconstrucci√≥n 3D del enc√©falo requiere de realizar un examen CT de Tomograf√≠a Computada al paciente para as√≠ obtener "capas" o secciones de la cabeza del individuo. Mediante sistemas convencionales se suelen extraer entre 100 y 300 capas por paciente, ya que n√∫meros superiores implicar√≠an una prolongaci√≥n de la exposici√≥n del paciente a radiaci√≥n ionizante.
Mediante el sistema CT2CT (basado en la tecnolog√≠a Pix2Pix), se consigue aumentar radicalmente el n√∫mero de capas internas del enc√©falo con tan solo el escaneo de unas "capas clave" que permiten al modelo predecir las partes del cerebro que no se han escaneado. De esta manera se reduce el tiempo de exposici√≥n del paciente a la radiaci√≥n y se puede llegar a aumentar el n√∫mero de capas 2D incluso 10 veces el n√∫mero de frames originales.

Esc√°ner original (240 frames) | Capas originales + Predecidas (480 frames, 2x original)
---------------------- | -----------------------------
![Esc√°ner original gif](/assets/original.gif) | ![Esc√°ner predecido gif](/assets/results.gif)

## ‚öôÔ∏èComo funciona?
El modelo CT2CT obtiene capas intermedias de escaneos CT desconocidas para aumentar la resoluci√≥n de las reconstrucciones 3D encef√°licas sin necesidad de aumentar el tiempo de los ex√°menes m√©dicos y, por ende, de exposici√≥n a la radiaci√≥n.
Una vez el entrenamiento se ha realizado, se puede pedir al modelo predicciones tantas como se deseen (teniendo en cuenta que el error se va aumentando conforme se predicen capas).

## üèãÔ∏èEntrenamiento
El entrenamiento funciona escogiendo 3 secciones conocidas (A, B y C), el input para el generador es A uni√≥n C (AuC) y el output se compara con el discriminador con B.

Capa A | Capa B | Capa C
-------|--------|--------
![Capa A](/assets/A.jpg) | ![Capa B](/assets/B.jpg) | ![Capa C](/assets/C.jpg)

De esta manera, el generador aprende a crear im√°genes de capas intermedias del enc√©falo ya que la informaci√≥n de las capas A y C est√° condensada en la uni√≥n de ambas im√°genes (AuC).

Input AuC | Predicci√≥n (10 epochs)[X] | Predicci√≥n (50 epochs)[X]
----------|-------------------------|------------------------
![Imagen AuC](/assets/AmC.jpg) | ![Predicci√≥n 10 epochs](/assets/B_10e.jpg) | ![Predicci√≥n 50 epochs](/assets/B_50e.jpg)

[X]: Generado por otro AuC no mostrado aqu√≠.

El modelo ha sido dise√±ado para que solo requiera de tan solo 80 im√°genes de entrenamiento para que sea √∫til para la gran mayor√≠a de escaneos CT. 50 epochs han demostrado una gran capacidad de predicci√≥n.

## ‚úÖEvaluaci√≥n
CT2CT utiliza el 20% del dataset para la evaluaci√≥n del modelo que jam√°s han sido previamente conocidas extra√≠das del propio esc√°ner original.

## üßîComo puedo usarlo yo?
Por el momento no es f√°cil de utilizar tus propios scans a no ser que entiendas el c√≥digo. Esta funcionalidad ser√° a√±adida en un futuro cercando pero puedes seguir la gu√≠a paso-por-paso abriendo CT2CT.ipyinb en Google Collaboratory para entrenar un modelo, ver los resultados y guardar el modelo entrenado.
Ahora mismo estoy desarrollando la aplicaci√≥n de escritorio que permitir√° conseguir un scan predecido al completo.

## ‚ùóGoogle Colaboratory
El c√≥digo est√° programado en Google Colaboratory. Si desesa utilizar este software, abra CT2CT.ipynb en Google Colaboratory.

## ‚ÑπÔ∏èInformaci√≥n Extra
En el proceso del desarollo de este software han surgido algunos temas de inter√©s para el futuro:
  Imagen 1: Al juntar todas las capas predecidas de un paciente se puede obtener un esc√°ner de igual resoluci√≥n que el original con la distinci√≥n de que el enc√©falo que se representa NO EXISTE! Es un tema interesante ya que la cabeza que se podr√≠a extraer en 3D ser√≠a del paciente pero al mismo tiempo no es 100% real? Interesante sin m√°s.
  La recreaci√≥n 3D de las facciones de la cara ser√≠an otro tema de inter√©s ya que se tratar√≠a de un rostro que NO EXISTE pese a que est√° basado en el del paciente original.
  Imagen 2: Si las im√°genes del esc√°ner original tienen un HUD descriptivo, el software lo tomar√° como parte de la imagen a predecir e intentar√° predecir tambi√©n el texto que la rodea. De esta manera, datos que no cambian y que se muestran, como el nombre del paciente o la fecha del examen son reproducidos a la perfecci√≥n, pero datos como el n√∫mero de capa o los mil√≠metros avanzados se ven de manera borrosa ya que no existen esos datos pues son desconocidos para el software y para el humano.
  
  | Imagen 1 | Imagen 2 |
  | --- | --- |
  | Reconstrucci√≥n 3D Predecida | HUD "borroso" (esquina sup. izqda.) |
  | ![Cara predecida](/assets/face.jpg) | ![HUD borroso](/assets/B_prediction.jpg) |
