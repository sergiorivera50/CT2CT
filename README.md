# CT2CT by Sergio Rivera - 16/10/2019

- If you want to read the Spanish version click [here](#spanish-version)
- Si quieres leer la versi√≥n en Espa√±ol haz click [aqu√≠](#spanish-version)

This project participated and was mentioned in the AI YouTube channel DotCSV! [link](https://youtu.be/BerOC6n8j9Q?t=1011)

# üíÇ‚Äç‚ôÇÔ∏èENGLISH VERSION
# CT2CT: Resolution Augmentation of Computed Tomography (CT) Scans

## üìùAbstract
The reconstruction of 3D encephala requires to perform the patient a Computed Tomography exam so that "slices" or sections of their head are obtained. The use of conventional systems usually extracts between 100 and 300 slices per patient, higher numbers would imply a longer exposure of the patient to major levels of radiation.
Using the CT2CT system (based on Pix2Pix technology), it is now achieved a higher number of inner slices of the encephalon only by scanning specific "key slices" that allow the model to predict the non-scanned slices in between them. This way, the time of exposure to radiation gets reduced significantly and the number of 2D slices raises for a better reconstruction of the 3D representation of the patient's head.

## ‚öôÔ∏èHow it works?
The CT2CT model obtains un-scanned intermediate layers from known scanned ones in order to augment the resolution of the 3D reconstruction of enchapla without requiring the patient to stay longer times exposed to radiation during medical exams.
Medicine is a field where accuracy is of extreme importance, the CT2CT model has to be retrained every time the target patient changes. This facilitates the AI to adapt to the new intrinsic patterns of the nature of the encephalon of the new patient without mixing previous brain structures of other patients. Once the training has been completed for one target patient, predictions of the brain structure of that patient can be asked to the model as many times as required (bearing in mind that little errors always add up) without having to repeat training constantly.

## üèãÔ∏èTraining
The training proccess works by choosing 3 known sections (A, B and C), the input for the generator is X (AxC) and the output obtained is compared with the discriminator unit to B.

Layer A | Layer B | Layer C
-------|--------|--------
![Layer A](/assets/A.jpg) | ![Layer B](/assets/B.jpg) | ![Layer C](/assets/C.jpg)

This way, the generator learns how to create images of intermediate slices of the encephalon since the information of A and C is condensed in the multiplication of both images (X).

Input AxC | Prediction (100 epochs) | Prediction (500 epochs)
----------|-------------------------|------------------------
![Image AxC](/assets/AxC.jpg) | ![Prediction](/assets/B_prediction.jpg) | Image Not Available

The model has been designed so that it only requires less than 100 images (all results shown have been generated with only 80 images of training). 500 epochs have demonstrated a huge capacity of prediction although the only drawback is the computational time. Said training was on-going for at least 12 hours, which is totally recommended if you need to generate a high accuracy scan. In contrary, if you only want the resolution of the 3D reconstruction to agument for visual purposes, 100 epochs are more than enough.

## ‚úÖEvaluation
CT2CT uses 20% (20 images in shown results) of the images of the dataset for evaluating the model.

## üî¢Parameters
All parameters, like "PATHS", "epochs" or "slices" can be modified in the first lines of code (explained through comments). The project already comes a default scanner with 240 slices which has been used during the whole development of the model.

## üßîHow can I use CT2CT?
It is recommended to use the default scanner (rawIMG, download zip folder included in this repository and upload the images to Drive, if you don't want to touch the code upload it to a folder like this: /brain_ct/PATIENT/rawIMG, this path is registered in code and can be changed in the variable RAW_PATH). If you prefer to use your own scans then this instructions must be followed:
  1. Add the images to the rawIMG folder under the names /IMG-0001-XXXXX.jpg being X the number of the slice (i.e. /IMG-0001-01234.jpg).
  2. Change the parameter "slices" to the number of images previously uploaded to the rawIMG folder in Drive.

The software outputs a completely new scan generated in brain_ct/results. 
This is the final output (known slices with predictions in between).

## Current folder structure
```
brain_ct (master folder)
  |  
  |__ generatedScan (B_prediction + C_prediction + ...)
  |
  |__ results (definitive output, A + B_prediction + B + C_prediction + ...)
  |
  |__ scan_dataset (temporary storage of X = A x B)
  |
  |__ PATIENT
        |
        |__ rawIMG (/IMG-0001-XXXXX.jpg original scan)
        |
        |__ inputIMG (X = A x B)
        |
        |__ outputIMG (B + C + ...)
        |
        |__ attempts (test dataset results, 5 random frames)
```
## ‚ùóGoogle Colaboratory
The code has been written in Google Colaboratory. Future updates will come in order to improve the user experience and try to make it user-friendly having in mind non-developers.
These are the parameters of the environment:
  1. Execution environment -> Change type of execution environment -> GPU
  2. If the training is halted by the automatic disconnection of the user from session, I recomend using an autoclicker so that Google thinks you are actually doing something with the code and, therefore, not being kicked.

## ‚ÑπÔ∏èExtra information
During the process of the development of this software some interesting ideas have been raised for the future:
  Image 1: When all the predicted layers are joined together, a scanner of similar resolution of the original is obtained with the only difference that the represented encephalon DOES NOT EXIST. This is an interesting idea since the head that could be extracted in 3D would be based on the original patient's head, but completely new. Face could also be reconstructed and would also be based on a person but not exactly THAT person.
  Image 2: If the images from the dataset contain descriptive HUD, the software would understand it as part of the image and will try to also predict that text. This way, data that does not change and is shown, such as the name of the patient or the date of the exam, are reproduced perfectly. But data that is constantly changing like slice number or distance are predicted in a blury way. This happens because that piece of information is unkown to the software and to the user.
  
  | Image 1 | Image 2 |
  | --- | --- |
  | 3D Predicted Reconstruction | "Blury" HUD (upper left corner) |
  | ![Predicted face](/assets/face.jpg) | ![blury HUD](/assets/B_prediction.jpg) |
  
# üíÉSPANISH VERSION
# CT2CT: Aumentaci√≥n de la Resoluci√≥n de Esc√°neres de Tomograf√≠a Computada (CT)

## üìùAbstracto
La reconstrucci√≥n 3D de enc√©falos requiere de realizar un examen CT de Tomograf√≠a Computada al paciente para as√≠ obtener "capas" o secciones de la cabeza del individuo. Mediante sistemas convencionales se suelen extraer entre 100 y 300 capas por paciente, ya que n√∫meros superiores implicar√≠an una prolongaci√≥n de la exposici√≥n del paciente a niveles de radiaci√≥n importantes.
Mediante el sistema CT2CT (basado en la tecnolog√≠a Pix2Pix), se consigue aumentar radicalmente el n√∫mero de capas internas del enc√©falo con tan solo el escaneo de unos "key slices" o capas clave que permiten al modelo predecir las capas que no se han escaneado. De esta manera se reduce el tiempo de exposici√≥n del paciente a la radiaci√≥n y se aumenta el n√∫mero de capas 2D para la reconstrucci√≥n 3D del enc√©falo.

## ‚öôÔ∏èComo funciona?
El modelo CT2CT obtiene capas intermedias de escaneos CT desconocidas para aumentar la resoluci√≥n de las reconstrucciones 3D encef√°licas sin necesidad de aumentar el tiempo de los ex√°menes m√©dicos y, por ende, de exposici√≥n a la radiaci√≥n.
Ya que en el campo de la medicina la precisi√≥n es de extrema importancia, el entrenamiento del modelo se ha de repetir cada vez que se trate con un nuevo paciente. As√≠ se consigue que la IA se adapte a los patrones espec√≠ficos de cada enc√©falo sin mezclarlos entre pacientes.
Una vez el entrenamiento se ha realizado, se puede pedir al modelo predicciones tantas como se deseen de manera que no es necesario repetir el proceso constantemente.

## üèãÔ∏èEntrenamiento
El entrenamiento funciona escogiendo 3 secciones conocidas (A, B y C), el input para el generador es X (AxC) y el output se compara con el discriminador con B.

Capa A | Capa B | Capa C
-------|--------|--------
![Capa A](/assets/A.jpg) | ![Capa B](/assets/B.jpg) | ![Capa C](/assets/C.jpg)

De esta manera, el generador aprende a crear im√°genes de capas intermedias del enc√©falo ya que la informaci√≥n de las capas A y C est√° condensada en la multiplicaci√≥n de ambas im√°genes (X).

Input AxC | Predicci√≥n (100 epochs) | Predicci√≥n (500 epochs)
----------|-------------------------|------------------------
![Imagen AxC](/assets/AxC.jpg) | ![Predicci√≥n](/assets/B_prediction.jpg) | Imagen No Disponible

El modelo ha sido dise√±ado para que solo requiera de tan solo 80 im√°genes de entrenamiento para que sea √∫til para la gran mayor√≠a de escaneos CT. 500 epochs han demostrado una gran capacidad de predicci√≥n aunque el √∫nico inconveniente es el tiempo de computaci√≥n. Dicho entrenamiento tard√≥ una noche aproximadamente, lo cual es totalmente recomendable si se necesita generar un esc√°ner de alta precisi√≥n.
Por el contrario, si tan solo se quiere aumentar la resoluci√≥n de la reconstrucci√≥n 3D de forma visual, 100 epochs son m√°s que suficientes.

## ‚úÖEvaluaci√≥n
CT2CT utiliza 20 im√°genes para la evaluaci√≥n del modelo que jam√°s han sido previamente conocidas extra√≠das del propio esc√°ner original.

## üî¢Par√°metros
Todos los par√°metros como los PATHS, epochs o slices del escaner pueden ser modificados en las primeras celdas del c√≥digo (explicado mediante comentarios). El proyecto viene con un esc√°ner por defecto de 240 capas o "slices" y es el que se ha utilizado en todo momento para el desarrollo del modelo.

## üßîComo puedo usarlo yo?
Es recomendable utilizar el escaner por defecto (rawIMG, descargar carpeta zip incluida en el repositorio y subir a Drive las im√°genes al RAW_PATH: /brain_ct/PATIENT/rawIMG) ya que esto es un proyecto no comercial. Pero si se desea hacer pruebas con otros esc√°neres habr√° que seguir las siguientes instrucciones:
  1. A√±adir las im√°genes a la carpeta rawIMG bajo el nombre /IMG-0001-XXXXX.jpg siendo X el n√∫mero de capa (ej, /IMG-0001-01234.jpg)
  2. Cambiar el par√°metro "slices" al n√∫mero de im√°genes previamente introducidas en la carpeta rawIMG.

El software obtiene como output un esc√°ner totalmente generado en brain_ct/results.

## Estructura actual de los archivos
```
brain_ct (carpeta madre)
  |  
  |__ generatedScan (B_predecida + C_predecida + ...)
  | 
  |__ results (output definitivo, A + B_predecida + B + C_predecida + ...)
  | 
  |__ scan_dataset (almacenamiento temporal de X = A x B)
  | 
  |__ PATIENT
        | 
        |__ rawIMG (/IMG-0001-XXXXX.jpg esc√°ner original)
        | 
        |__ inputIMG (X = A x B)
        | 
        |__ outputIMG (B + C + ...)
        |  
        |__ attempts (resultado del dataset test, 5 frames aleatorios)
```
## ‚ùóGoogle Colaboratory
El c√≥digo est√° programado para que su uso sea en el entorno de Google Colaboratory. En el futuro se realizar√°n actualizaciones para mejorar la experiencia y que sea m√°s facil el uso del software para usuarios no-desarrolladores.
Estos son los par√°metros del entorno:
  1. Entorno de ejecuci√≥n -> Cambiar tipo de entorno de ejecuci√≥n -> GPU
  2. En caso de que en el entrenamiento desconecte al usuario de la sesi√≥n, recomiendo el uso de un autoclicker para que no cese la     actividad y google no te expulse del software.

## ‚ÑπÔ∏èInformaci√≥n Extra
En el proceso del desarollo de este software han surgido algunos temas de inter√©s para el futuro:
  Imagen 1: Al juntar todas las capas predecidas de un paciente se puede obtener un esc√°ner de igual resoluci√≥n que el original con la distinci√≥n de que el enc√©falo que se representa NO EXISTE! Es un tema interesante ya que la cabeza que se podr√≠a extraer en 3D ser√≠a del paciente pero al mismo tiempo no es 100% real? Interesante sin m√°s.
  La recreaci√≥n 3D de las facciones de la cara ser√≠an otro tema de inter√©s ya que se tratar√≠a de un rostro que NO EXISTE pese a que est√° basado en el del paciente original.
  Imagen 2: Si las im√°genes del esc√°ner original tienen un HUD descriptivo, el software lo tomar√° como parte de la imagen a predecir e intentar√° predecir tambi√©n el texto que la rodea. De esta manera, datos que no cambian y que se muestran, como el nombre del paciente o la fecha del examen son reproducidos a la perfecci√≥n, pero datos como el n√∫mero de capa o los mil√≠metros avanzados se ven de manera borrosa ya que no existen esos datos pues son desconocidos para el software y para el humano.
  
  | Imagen 1 | Imagen 2 |
  | --- | --- |
  | Reconstrucci√≥n 3D Predecida | HUD "borroso" (esquina sup. izqda.) |
  | ![Cara predecida](/assets/face.jpg) | ![HUD borroso](/assets/B_prediction.jpg) |
